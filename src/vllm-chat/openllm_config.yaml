# project: vllm-chat
# service_config:
#   name: llama2
#   traffic:
#     timeout: 300
#   resources:
#     gpu: 1
#     gpu_type: nvidia-rtx-3060
# engine_config:
#   model: TheBloke/Llama-2-7B-Chat-AWQ
#   max_model_len: 1024
#   quantization: awq
#   enforce_eager: true
# chat_template: llama-2-chat

chat_template: chatml
engine_config:
  dtype: half
  max_model_len: 2048
  model: HHPAI-BSC/Qwen2.5-Aloe-Beta-7B
extra_labels:
  model_name: HHPAI-BSC/Qwen2.5-Aloe-Beta-7B
  openllm_alias: 7b,7b-instruct
project: vllm-chat
service_config:
  name: qwen2.5
  resources:
    gpu: 1
    gpu_type: nvidia-T4
  traffic:
    timeout: 300
